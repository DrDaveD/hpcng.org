{"componentChunkName":"component---src-templates-blog-post-jsx","path":"/community-charter/","result":{"data":{"site":{"siteMetadata":{"name":null,"title":"HPCng","description":"HPCng is an open community of people and organizations interested in the broad modernization of HPC capabilities across a wide range of use-cases ranging from traditional HPC to enterprise and hyper-scale workloads."}},"markdownRemark":{"id":"9f7556d1-41c9-5220-86d2-5dc03be0e429","excerpt":"Before you read this… This document is a draft! Please comment or suggest changes on GitHub, or send an email to the HPCng organization lead Gregory Kurtzer at…","html":"<p><strong>Before you read this…</strong></p>\n<p><strong>This document is a draft!</strong> Please comment or suggest changes on <a href=\"https://github.com/hpcng/hpcng.org/blob/master/content/pages/community-charter.md\">GitHub</a>, or send an email to the HPCng organization lead Gregory Kurtzer at gmk@hpcng.org.</p>\n<h2>Purpose</h2>\n<p>The purpose of this document is to introduce you to, and better define, the Next Generation of High-Performance Computing (HPCng) community, including its community and development goals. This document is a work in progress and feedback, comments, and suggestions are encouraged.</p>\n<h2>Background</h2>\n<p>High-Performance Computing (HPC), a once niche use-case, has been growing in terms of applicability over the last several years. We are seeing large scale, tightly integrated HPC resources beginning to run non-traditional HPC workloads in what we commonly refer to as “the long tail of scientific computing.” As that long tail is growing, it is becoming increasingly common for new applications, use-cases, and science domains to be making use of traditional high-performance computing infrastructures. Additionally, HPC adoption is growing beyond traditional scientific use-cases and into the enterprise through the advent of artificial intelligence training as well as compute and data-driven analytics.</p>\n<p>When we refer to HPC today, we no longer are referring only to those tightly coupled parallel Message Passing Interface (MPI) based applications. We are referring to any application, or series of applications, which are designed to run a given workload as fast as the hardware will allow. This usually means that the performance-critical application will utilize a given subsystem of a hardware stack completely, whether that be CPU, memory, storage, network, GPU, PCI, etc. This could be on a single compute node, a massive GPU workhorse, CPU farm, or a tightly coupled parallel processing infrastructure. For the purpose of this charter, HPC inclusively refers to all of these types of workloads as HPC.</p>\n<p>While “HPC” is becoming increasingly diverse, the architecture to which it is built has not changed considerably over the past two-and-a half decades. This base architecture, commonly referred to as the <a href=\"https://en.wikipedia.org/wiki/Beowulf_cluster\">“Beowulf”</a> design, is predisposed to being flat, isolated, and discrete, which has imposed difficulties with integration of non-HPC specific capabilities like, orchestration, CI/CD and DevOps integration, security process, accountability, and data management (more information below).</p>\n<p>To loosely quote an anonymous venture capitalist:</p>\n<blockquote>\n<p>There are clearly two parties going on: one in HPC, and one in enterprise.</p>\n</blockquote>\n<p>This divide has been increasingly causing separation as enterprise architectures, like micro-services, containers, orchestration, etc., have been readily innovating over the past decade and creating a lot of opportunities for optimization of development, operations, security, automation, reliability, and scale. But these new capabilities are not easily transcribed into the HPC sector due to misalignments of base architectures, and thus cross-pollination is not occurring. This leads to the replication of technology and a lack of benefit of experience and capabilities for everyone. </p>\n<p>HPCng strives to build a community of diverse brilliant people to consider these challenges and work together on solutions. </p>\n<h2>Goals</h2>\n<h3>Community</h3>\n<p>We, the members of the HPCng community, strive to create a diverse community of backgrounds, ideas, skill sets, and perspectives. Such diverse perspectives are needed to unite the currently disparate capabilities of HPC and enterprise to allow for running advanced computational and data analytics at scale. </p>\n<p>Engaging with these respective communities will enable a greater understanding of requirements as well as existing technologies that could be used as part of this vision. This will facilitate cross-pollination to create technologies and solve problems across the ecosystem related to incompatible technologies and architectures between traditional Enterprise IT and HPC.</p>\n<h3>Collaboration</h3>\n<p>Referring back to the two party analogy in the abovementioned background, very few people intermingle between these parties, even when a single organization (like oil and gas, pharmaceuticals, aerospace, etc.) hosts both parties. Further, it could be said that the misalignment between HPC and enterprise is detrimental to both communities. On the other hand, there is value both can offer to each other by leveraging the best capabilities across the ecosystem. Thus, we strive to unite these parties and technologies where it makes sense, create new technologies and methodologies as needed, and build the appropriate bridges. We believe that the best way to build these capabilities is not by corporate direction, but rather led by a community of people that this directly affects, where everyone is a stakeholder.</p>\n<h3>Goals</h3>\n<p>Using the above metaphor, while the technical expertise between both parties is obviously quite specialized, it is important for these parties to benefit from each other’s experience, capabilities, and technologies when applicable. There is an outcome here where some amount of infrastructure can be easily shared between enterprise use cases (AI/ML, inference, compute and data driven analytics, etc.) as well as HPC and computational use cases (simulation, modeling, rendering, prediction, analysis, research, and science).</p>\n<blockquote>\n<p>“The convergence of AI, data analytics and traditional simulation will result in systems with broader capabilities and configurability as well as cross pollination.” —  Dr. Al Gara, Intel</p>\n</blockquote>\n<h3>Projects</h3>\n<p>We strive to develop a collection of open source, community driven projects that represent the next generation of high performance computing system infrastructure. As such infrastructure naturally spans beyond a single utility, the HPCng effort is split into several projects and focus areas. </p>\n<h2>Projects / Focus Areas</h2>\n<p>Moving forward, there are many opportunities to further modernize HPC. The projects and ideas stated here are only designed as a starting point and will surely develop over time through the involvement of stakeholders, technology advancements, and identified needs. The following projects (existing and not yet started) represent the scope of ideas that are to be worked on by HPCng. </p>\n<h3>Workflows</h3>\n<p>HPC has moved beyond just traditional HPC and the “Long Tail of Science” is now swallowing the system. As a result, there is an increasing variety of different kinds of work being done on today’s HPC clusters. For example, there are traditional tightly-coupled parallel jobs, loose coupled parallel jobs, threaded, serial, multiple variants of MPI, services, dependencies, pipelines, dependencies, job arrays, etc.</p>\n<p>The next generation of HPC infrastructure must be able to handle these diverse workloads, dependencies, and job and data orchestration to enable both traditional parallel jobs and non-traditional HPC jobs.</p>\n<h3>Orchestration</h3>\n<p>There are several orchestration systems being widely utilized for enterprise, some of which are also being considered for performance critical workloads (e.g., Kubernetes and Nomad). There are two ways to consider orchestration of workloads:</p>\n<ul>\n<li><strong>Running HPC workloads directly through the orchestrator:</strong> There are a number of people considering running HPC workloads directly from Kubernetes, most of which have only been successful for the most rudimentary of workloads and scheduling policies. There is clearly a lot of work to do to achieve full parity with existing HPC requirements for scheduling and workload management. Strategy is needed here.</li>\n<li><strong>Meta-scheduling HPC with an orchestrator:</strong> It is possible to use an orchestrator to implement meta-scheduling of HPC workloads to multiple clusters and workload management systems like Slurm, PBS, Grid Engine, etc. Several such projects are currently known for this (if there are more, let us know): Multicluster-Scheduler, HTCondor, and WLM-Operator.</li>\n</ul>\n<h3>Singularity/Containers</h3>\n<p>Singularity is a fantastic initial step in the modernization of HPC as it allows containers (a technology derived from enterprise focused innovation) to be leveraged and extended upon within traditional HPC contexts. Singularity, while popular and widespread in the community, is not the only container system being used. Other container runtimes including Shifter, CharlieCloud, and Sarus, are also utilized in various other scenarios. While container runtimes are an important building block in the next generation of HPC, they are a single piece of the puzzle.</p>\n<p>Please note: Singularity has already been moved to the HPCng community GitHub organization and is now available at: <a href=\"https://github.com/hpcng/singularity/\">https://github.com/hpcng/singularity/</a></p>\n<h3>Cluster Management and Provisioning</h3>\n<p>There are multiple ways to manage HPC clusters, (Warewulf being one of them that HPC centers have direct experience with) but they do not currently solve all of the problems that we are experiencing with both large and small clusters, nor do they integrate well into enterprise computing environments for the same reasons that legacy HPC misaligns with enterprise architectures. It is our goal to help and modernize this initiative.</p>\n<p>Warewulf has not yet moved into HPCng, but we are hoping to have that occur very soon now.</p>\n<h3>Public Key Infrastructure</h3>\n<p>Singularity supports its own key management paradigm and leverages standard HKP (Horowitz Keyserver Protocol), but it needs to be extended to completely support Synchronizing Key Server (SKS) Pools and the Notary container standard currently being developed.</p>\n<h3>Secrets Management</h3>\n<p>Encryption of containers is possible with Singularity, and due to its container image format, SIF, when used, can run containers directly from an encrypted format. While this can already be used with Singularity to manage secrets and decryption, it is not well integrated and can not operate well with orchestration systems today like cloud hosted Kubernetes.</p>\n<h3>CI/CD Integration</h3>\n<p>Continuous Integration and Continuous Deployment (CI/CD) is a method of automating, accelerating, and testing software from SCM (Source Code Management) to production. Containers are the default portable output format (artifact) from these CI/CD pipelines and are currently being used widely in enterprise deployments.</p>\n<p>Integrating this process into HPC workflows can bring similar benefits to HPC environments as well as providing trusted software stacks leveraging cryptographic validation on all workloads that come out of the CI/CD pipeline providing 100% trust and validity.</p>\n<h3>Hybrid-Location Federated HPC</h3>\n<p>More and more, people are interested in leveraging additional resources for computing involving multiple systems, on-prem, off-prem, cloud, multi-cloud, etc. We have various capabilities being offered in the cloud (AWS, Azure Batch, Rescale, Ubercloud, Atrio, etc.), but we don’t have a suitable mechanism for making workloads or data hybrid-capable. Data platforms like Globus, RStor, OpenDrives, Wasabi, as well as self-hosted deployments of Ceph and/or Minio could be a suitable backend for global data migration, but we still need a way to tie that directly to the workloads.  </p>\n<p>Another consideration is that once many locations are involved in handling a single workload, another immediate issue that arises is a common, secure software distribution platform that performs well over a wide area, and scales to very large numbers of nodes. This can be done via a distributed file system (perhaps CVMFS as an option) or a distributed caching system which works in conjunction with the orchestration and workflow engine. </p>\n<h3>Reproducibility/Trust</h3>\n<p>Reproducibility in science is a critical problem to solve so we can have confidence in the software environments that we are running in. While we can’t solve it for the entire stack (e.g. hardware), we can solve it for the software stack. The answer to reproducibility and compliance is trust and assurance.</p>\n<p>The ability to tag a workload with a cryptographic signature allows for not only validation but also accountability. Containers, being the basic building block moving forward, allows us to gain strong confidence in the environments we are running within, but we are not completely there yet.</p>\n<p>We still need the control surface necessary to manage not only the containers, but also the cryptographic and secure materials needed to guarantee trust.</p>\n<p>Note, this is related to the PKI focus area above but offers additional capabilities around computational use cases and controls compliance like FDA and FAA software stack regulations.</p>\n<h3>Composable Hardware</h3>\n<p>The HPC of the future will consist of not only complex schedulers, software stacks, and data flows, but many different hardware components, ranging from continuing evolution of CPUs and GPUs to more domain-specific accelerators purpose built to support complex computational workflows. HPC must be able to effectively and dynamically “compose” complex hardware topologies leveraging next generation memory semantic fabrics, such as GenZ and CXL. </p>\n<h3>MPI and InfiniBand Container integration</h3>\n<p>While leveraging MPI through containers has been largely accomplished and MPI is now moving into enterprise workloads like AI training via Horovod, there are a number of areas that can still be further addressed for building, running, and maintaining MPI based containers. Additionally, the libraries and host/kernel dependencies and Application Binary Interfaces (ABI) communication pathways (e.g., OFED) still need to be more universally addressed.</p>\n<h3>OCP/Hyper-scale</h3>\n<p>There have been significant advances in the hyper-scale space for very efficient and scalable hardware designs, but almost nobody in HPC is leveraging these capabilities. It is advantageous for HPC to be part of the OCP community, and the hyper-scale “scale-up” architecture is highly advantageous, and can (in theory) be used very well for HPC use-cases.</p>\n<h3>Service Mesh and Service Discovery</h3>\n<p>There is much room for improvement in the traditional HPC space for Service Mesh (think Istio and Hashicorp’s Consul) and Service Discovery. These technologies could be adopted/augmented and leveraged to provide container, service information, and intelligence (health, capabilities, address information, etc.) to enhance the capabilities of HPC, allowing for self-healing and scaling, among other uses. </p>\n<h3>Storage Agnostic Layers</h3>\n<p>There is a need for HPCng to have a broad support for different storage methodologies and types in order to gain maximum scalability and functionality. Storage considerations for HPC are usually designed for the task at hand, rather than general purpose. HPC 2.0 needs to be able to ingest and consume storage of many disparate types and present them in a way (perhaps using a service mesh) to jobs that are run to match the appropriate storage to its job type. This is sometimes leveraged through job schedulers such as SLURM via <code class=\"language-text\">--hint</code> style options for CPU architectures — this type of design could be implemented for storage as well. </p>\n<p>There are many more initiatives that still need to be identified, as well as goals for this community. This is only meant as a starting point and should be fleshed out with time and collaboration. Please add comments to any of the above ideas, or feel free to write your own to the list.</p>\n<h2>Corporate Structure</h2>\n<p>The goal of HPCng is to become a tax exempt non-profit, both in the US as well as abroad (e.g., Switzerland). This is a long and arduous process here in the US but this is the shared goal of the HPCng Board of Directors.</p>\n<h3>Board of Directors</h3>\n<p>As of the first draft of this document, HPCng has been founded and led by a sole individual, Gregory M. Kurtzer. Moving forward, Kurtzer has identified the necessity for the leadership to be more than that of a single individual or single company. To this point, Kurtzer has created an HPCng Board of Directors (HBOD) which will be used to share the leadership and decision making responsibilities to an initially invited board.</p>\n<p>At the time of this writing, the confirmed board members are as follows:</p>\n<ul>\n<li>Andrew Younge, PhD: Computer Scientist, Sandia National Laboratories, DOE</li>\n<li>Brent Gorda: Senior Director of HPC at ARM, Inc.</li>\n<li>Brock Taylor: Director of HPC Solutions at AMD, Inc.</li>\n<li>Glen Otero, PhD: Vice President of Scientific Computing, TGen</li>\n</ul>\n<p>With Gregory M. Kurtzer taking the role of Chairman of the Board.</p>\n<p>The roles of the HBOD are as follows (but not limited to therein, as the structure is still developing):</p>\n<ul>\n<li>Establish vision, mission, and values of HPCng</li>\n<li>Approve, agree, and amend the necessary documents (including this charter)</li>\n<li>Be an escalation point of authority for decision making process when necessary</li>\n<li>Define and maintain the board members themselves</li>\n<li>Appoint the leadership of HPCng whom will execute the vision, mission, and values of the organization</li>\n<li>Ensure proper communication and transparency in being an open community and organization\nHandle the establishment of the financial and legal structure of HPCng</li>\n</ul>\n<p>In summation, the HBOD is responsible for the high level structure, organization, and vision, not the day-to-day management of the organization.</p>\n<h3>Management Structure</h3>\n<p>As the HBOD is predominantly responsible for high level matters of HPCng, day-to-day responsibility will be undertaken by the management team.</p>\n<p>The roles and responsibilities of this management team is still being decided upon, but we are aware of some basic and critical roles already:</p>\n<ul>\n<li>Executive Director</li>\n<li>\n<p>Director of Operations</p>\n<ul>\n<li>Systems infrastructure management</li>\n</ul>\n</li>\n<li>Director of communications/PR</li>\n<li>Secretary/Admin </li>\n<li>Community Manager</li>\n</ul>\n<p>The roles and responsibilities of the management team are currently being discussed and evolving.</p>\n<h2>Discussions and collaboration</h2>\n<p>We will hold recurring meetings to facilitate collaboration on the above topics, discuss additional topics of interest, organize teams, presentations, and votes. Attendance to these meetings will be open to anybody who wishes to join, and everyone is encouraged to be part of these.</p>\n<p>Agendas will be posted and everybody can influence the discussions.</p>\n<h2>In the Nature of Open Source</h2>\n<p>Corporate involvement and contributions to HPCng will be essential to driving adoption and creating robust infrastructure for the future of HPC. To encourage and invite corporate support, there needs to be paths for products and services built on top of HPCng. In the same light, there must also be paths that do not require any proprietary or commercial products. This allows commercial entities, big and small, to participate and contribute to create a level playing field with opportunities for differentiating through innovation on top of HPCng. It also ensures that the base benefits of HPCng are freely available and provide significant value to all users without the need for commercial or proprietary components. With commercial extensions and service opportunities, HPCng can expect more inputs and contributions from technical and business resources from corporate entities in HPC. </p>\n<p>No open source project will have non-OSI licensed or restricted software or API requirements without an exemption from the HPCng Board of Directors (e.g. NVidia Cuda). HPCng will always leverage open API requirements and ensure that an open source path exists, even if that is a limited reference implementation. HPCng requires that all APIs and interfaces of hosted projects are open source and freely available (e.g. OSI approved licenses). The process for requesting an exemption will be determined at a later time.</p>\n<p>We ask that all other people and organizations who wish to be part of the HPCng community to do the same. Companies who benefit from the discussions and insights from the community should also release any resulting projects, software, and/or documentation which was inspired by this community as open source, preferably within this community’s GitHub organization. In doing so, the contributing leading organizations will be administrators of that project and have a delegate membership in the HPCng organization. All developers will also be able to leverage infrastructure that will be built up for all projects, and founders of projects will maintain authority.</p>\n<p>HPCng is designed to be community focused, and any participating corporations should not use HPCng as a marketing opportunity to drive products and sales, but rather as a platform to drive features, capabilities, and collaboration.</p>\n<h2>Security Protocol</h2>\n<p>There are challenges in the open source community with how best to address and resolve security issues. This is a rough guideline and will be further fleshed out on how we will handle security issues for any of the HPCng projects:</p>\n<ol>\n<li>Security report is made to the HPCng security team members</li>\n<li>Security diligence will be done and severity of the issue will be ascertained</li>\n<li>If the issue is not deemed to have security implications, this process stops and embargo is lifted</li>\n<li>The HPCng security team (and the reporter) will work together to find a reasonable solution and/or mitigation</li>\n<li>Project security stakeholders will all be under contracted embargo to keep security context and information confidential until a full public release and disclosure is made (note: a security team vendor could in fact make a fix before one is agreed upon and release that without a security disclosure to their commercial user base)</li>\n<li>The release of the fix and lift of embargo will occur after sufficient testing and agreement by the security team stakeholders (note: releases for zero-day exploits will be expedited)</li>\n<li>CVEs will be obtained for security releases</li>\n</ol>\n<p>If you wish to be on the HPCng security team, or have a security issue to discuss for any of the HPCng projects, please contact the HPCng security team at security@hpcng.org.</p>\n<h2>Challenges</h2>\n<p>There are some challenges that are already foreseen, and we’re sure more will arise. These include:</p>\n<ul>\n<li>Refining the early messaging (the text in this document)</li>\n<li>Getting the word out about what we are doing</li>\n<li>Engagement with the community</li>\n<li>Balancing the load (it shouldn’t be a small number of people doing all the work)</li>\n<li>Infrastructure management</li>\n<li>Volunteering</li>\n</ul>\n<p>This is where we will need help, discussions, and volunteers.</p>\n<h2>Call-To-Action</h2>\n<p>Join the HPCng community <a href=\"https://join.slack.com/t/hpcng/shared_invite/zt-qda4h1ls-OP0Uouq6sSmVE6i_0NrWdw\">Slack team</a>, join the channels you are interested in being part of, and introduce yourself to us!</p>","frontmatter":{"title":"Community Charter","date":null,"description":"The Next Generation of High-Performance Computing Community Charter"}}},"pageContext":{"slug":"/community-charter/","locale":"en","hrefLang":"en-US","originalPath":"/community-charter/","dateFormat":"MM/DD/YYYY"}},"staticQueryHashes":["1239077767","3000541721","3280999885","3280999885"]}